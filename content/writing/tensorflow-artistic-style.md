---
date: 2016-08-31T21:00:00+01:00
title: Convolutional neural networks for artistic style transfer
category: machine-learning
tags:
   - image-processing
   - convolutional-neural-networks
   - keras
   - tensorflow
includes_code: yes
includes_math: yes
---

There's an amazing app out right now called [Prisma][prisma] that
transforms your photos into works of art using the styles of famous
artwork and motifs. The app performs this style transfer with the help
of a branch of machine learning called [convolutional neural
networks][cnn-wikipedia]. In this article we're going to take a
journey through the world of convolutional neural networks from theory
to practice, as we systematically reproduce Prisma's core visual
effect as a [webapp][neural-style-implementation].

## So what is Prisma and how might it work?

Prisma is a mobile app (iPhone and Android at the time this piece was
written) that allows you to transfer the style of one image, say a
impressionist painting, onto the content of another, say a picture of
your baby. Here's a demo of the kind of images that it generates.

{{< figure src="//placehold.it/1440x960/f4bc87/ffffff" title="TODO: A three part image, showing the content image, the style image and the style-transferred image generated by Prisma." >}}

Like many people, I find much of the output of this app very pleasing,
and I got curious as to how it achieves its visual effect. At the
outset you can imagine that it's somehow extracting low level features
like the colour and texture of the brush strokes from one image (that
we'll call the *style image*) and applying it to more semantic, higher
level features like a baby's face on the other image (the *content
image*).

How would one even begin to do something like this? You could perhaps
do some pixel-level image analysis on the style image to get things
like spatially-averaged colours or maybe even aspects of its texture,
but how would you then *apply* these in a selective fashion that still
retains the essential aspects of the content image? And what about the
existing style of the content image?  How do we first *discard* this
before we apply the new style?

I was stumped by many of these questions really early on, and as one
does, turned to Google for help. My searches soon pointed me to a
really popular paper ([Gatys et al., 2015][neural-style-gatys-etal])
that explains exactly how all this is achieved. In particular, the
paper poses what we're trying to do in mathematical terms as an
optimisation problem:

Let $\mathbf{c}$ be the content image and $\mathbf{s}$ be the style
image. We're trying to generate an image $\mathbf{x}$ that minimises
the following *loss function*:

$$
\mathcal{L}(\mathbf{c}, \mathbf{s}, \mathbf{x}) =
\alpha \mathcal{L}_c(\mathbf{c}, \mathbf{x}) +
\beta \mathcal{L}_s(\mathbf{s}, \mathbf{x})
$$

where $\mathcal{L}_c(\mathbf{c}, \mathbf{x})$ is the *content loss* (a
function that grows as the generated image $\mathbf{x}$ "deviates in
content" from $\mathbf{c}$), and $\mathcal{L}_s(\mathbf{s}, \mathbf{x})$
is the *style loss* (a function that grows when the generated image
$\mathbf{x}$ "deviates in style" from $\mathbf{s}$). $\alpha$ and
$\beta$ are weighting factors that control how much we want to
emphasise the content relative to the style.

One crucial bit of insight in this paper is that the definitions of
these style and content losses *are not based on per-pixel
differences* between images, but instead in terms of higher level,
more *perceptual differences* between them. But then how does one go
about writing a program that understands enough about the *meaning* of
images to do something like this?

This question leads to the next crucial bit of insight.  *It's nearly
impossible to solve a general problem like this well with an a priori
fixed set of rules.* So what we instead need is an abstract learning
machine that, when fed with a bunch of raw example data, can
automatically discover the representations needed to solve arbitrary
problems. As luck would have it, a class of abstract learning machines
that are particularly well suited to dealing with images already
exist, and they're called *Convolutional Neural Networks* (CNNs).

Over the course of this article, we're going to learn a ton. For those
who are new to all of this, we begin with a [deep dive into the world
of CNNs][cnn-primer]. We then learn how to use [CNNs to solve the
problem posed by Gatys et al.][neural-style-algorithm] and reproduce
the visual effect of Prisma. As a bonus, we conclude with a [concrete
implementation of the solution][neural-style-implementation] (in Keras
and TensorFlow, and wrapped in a Django webapp!) that you can play
with and extend.

## Convolutional Neural Networks from the ground up

This section offers a brief summary of parts of the Stanford course
[Convolutional Neural Networks for Visual Recognition
(CS231n)][cs231n] that are relevant to our style transfer problem. If
you’re even vaguely interested in what you're reading here, you should
probably take this course. *It is outstanding*.

### The image classification problem

We begin our journey with a look at the *image classification*
problem. This is a deceptively simple problem to state: Given an input
image, have a computer automatically classify it into one of a fixed
set of categories, say "baby", "dog", "car" or "toothbrush". The
reason we're starting with this problem is that it's at the core of
many seemingly unrelated tasks in computer vision, including our quest
to reproduce Prisma's visual effect.

In more precise terms, imagine a three channel colour image (RGB)
that's $W$ pixels wide and $H$ pixels tall. This image can be
represented in a computer as an array of $W \times H \times 3$
integers, each going between $0$ (minimum brightness) and $255$
(maximum brightness). Let's further assume that we have $K$ categories
of things that we'd like to classify the image as being one of. The
task then is to come up with a function that takes as input one of
these large arrays of numbers, and outputs the correct label from our
set of categories, e.g. "baby".

{{< figure src="//placehold.it/1440x960/f4bc87/ffffff" title="TODO: The image classification problem." >}}

How might we write such a classification function? One naïve approach
would be to hardcode some characteristics of babies (such as large
heads, snotty noses, rounded cheeks, ...) into our function. But even
if you knew how to do this, what if you then wanted to look for cars?
What about different kinds of cars? What about worms? What if our set
of $K$ categories became arbitrarily large and nuanced?

To further complicate the problem, note that any slight change in the
situation under which the image was captured (illumination, viewpoint,
background clutter, ...) greatly affects the array of integers being
passed as input to our function. How do we write our classification
function to ignore these sorts of superfluous differences while still
giving it the ability to distinguish between a "baby" and a "small
child"?

Since this is starting to look hopeless on multiple fronts, we turn to
a completely different approach --- one that's more *data driven*. We
first gather a bunch of pre-classified images as examples and then
feed them into a *learning algorithm*. This algorithm uses the
examples to learn about the visual appearance of each class, allowing
it to automatically function as the classifier we want!

While this does sound amazing, it's also very hand-wavy. Let's make
things more concrete by taking a look at one of the simplest learning
image classifiers: A [*Softmax classifier* with a *cross-entropy*
loss][cs231n-softmax-classifier] function.

### A first learning image classifier

Recall the classification problem we're trying to solve. We have an
image $\mathbf{x}$ that's represented as a $D = W \times H \times 3$
array of numbers, and we want to find out which category (in a set of
$K$ categories) that it belongs to. One way to convey this information
is to get a *confidence score* for each category (and the largest
score amongst these will be the main category we're looking for).

In essence, what we're looking for is a function $f: \mathbb{R}^D
\mapsto \mathbb{R}^{K}$ that maps image data to class scores. The
simplest possible example for such a function is a linear map:

$$f(\mathbf{x}; \mathbf{W}, \mathbf{b}) = \mathbf{W}\mathbf{x} + \mathbf{b}$$

Here, the matrix $\mathbf{W}$ (of size $K \times D$) and the vector
$\mathbf{b}$ (of size $K \times 1$) are *parameters* of the
function. The algorithm will *learn* these with the help of the
*training* data that we have. (Which is a fancy way of saying that we
will fit them to the pre-classified example data.) For this, we
introduce a *loss* function $\mathcal{L}$ that quantifies the
disagreement between what our classifier suggests for the scores and
what our training data provides as the known truth. Thus, this loss
function goes up if the classifier is doing a poor job and goes down
if it's doing great. The parameters $\mathbf{W}$ and $\mathbf{b}$ are
tuned to minimise this loss --- another optimisation problem.

{{< figure src="//placehold.it/1440x960/f4bc87/ffffff" title="TODO: An image classifier showing the score function and the loss function" >}}

### Moving to neural networks

- TODO: Then, we move to a more complex, nonlinear version of the
  score function. But the rest of the ideas (loss, optimisation
  problem) stay the same.

{{< figure src="/images/writing/tensorflow-artistic-style/neural-network.svg" title="TODO: An example neural network image." >}}

### And finally, convolutional neural networks

- TODO: Turns out we can get much better performance with far fewer
  parameters by exploiting the fact that we have a 2D image.
- TODO: Summarise the Simonyan, et al. paper to motivate transfer
  learning.

## Returning to the style transfer optimisation problem

- TODO: Summarise the Gatys, et al. paper for the core ideas, problem
  statement and solution methodology. Point out that the loss function
  can be a CNN iteself!

## Implementation of the model in Keras and TensorFlow

- TODO: Step through the more crucial portions of the implementation
  on GitHub.
- TODO: Talk about how hyperparameters are tuned to improve aesthetic
  quality of the output. Show examples of things that work and things
  that do not.
- TODO: Mention the corresponding project functioning on GitHub.

## Serving the trained model as part of a webapp

But we want to make a near real time web service out of this, and so
we look for extensions of this algorithm.

- TODO: Introduce the Johnson paper for huge optimisation.
- TODO: Optimisation of and shortcuts to the implementation above to
  make it suitable for a user-facing app.
- TODO: Explain the theory behind exporting a learnt model from Keras
  (as TensorFlow data structures) and serving it; with TensorFlow
  serving?
- TODO: Step through important aspects of the implementation.

## Conclusion

- TODO: Repeat what we saw with some examples relating back to first
  motivating examples from Prisma.
- TODO: Reiterate some insights.
  - Maybe a giant array of pixels is not the best way of representing
    an image if we wish to understand it better.
  - Some classes of problems are hard to solve with a priori known
    rules, and require learning machines. -> Have them automatically
    discover the representations needed to solve arbitrary problems.
- TODO: Talk about ideas for extension and improvement.

## Selected references and further reading

1. [A Neural Algorithm of Artistic Style][neural-style-gatys-etal],
   the seminal article
2. [Very Deep Convolutional Networks for Large-Scale Image
   Recognition][vgg-simonyan-etal]
3. [Perceptual Real-Time Style
   Transfer][fast-neural-style-johnson-etal] and [supplementary
   material][fast-neural-style-johnson-etal-supp]
4. [Deep learning][deep-learning-review], a review in Nature
5. [Calculus on Computational Graphs: Backpropagation][backprop-explanation]
6. [The Stanford course on Convolutional Neural Networks][cs231n] and
   [accompanying notes][cs231n-notes]
7. [Our sample implementation on GitHub][neural-style-demo-project]
8. TensorFlow: [Deep CNNs][tensorflow-cnn], [GPU support on
   macOS][tensorflow-gpu-macos], [Serving][tensorflow-serving]
9. [Keras as a simplified interface to TensorFlow][keras-tensorflow]

[cnn-primer]: #convolutional-neural-networks-from-the-ground-up
[neural-style-implementation]: #serving-the-trained-model-as-part-of-a-webapp
[neural-style-algorithm]: #returning-to-the-style-transfer-optimisation-problem
[neural-style-demo-project]: https://github.com/hnarayanan/stylist
[prisma]: http://prisma-ai.com
[cnn-wikipedia]: https://en.wikipedia.org/wiki/Convolutional_neural_network
[neural-style-gatys-etal]: https://arxiv.org/abs/1508.06576
[fast-neural-style-johnson-etal]: https://arxiv.org/abs/1603.08155
[fast-neural-style-johnson-etal-supp]: https://cs.stanford.edu/people/jcjohns/papers/fast-style/fast-style-supp.pdf
[vgg-simonyan-etal]: https://arxiv.org/abs/1409.1556
[deep-learning-review]: https://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf
[backprop-explanation]: http://colah.github.io/posts/2015-08-Backprop/
[cs231n]: http://cs231n.stanford.edu
[cs231n-notes]: http://cs231n.github.io
[cs231n-softmax-classifier]: http://cs231n.github.io/linear-classify/#softmax-classifier
[tensorflow-cnn]: https://www.tensorflow.org/versions/r0.10/tutorials/deep_cnn/index.html
[tensorflow-gpu-macos]: https://gist.github.com/ageitgey/819a51afa4613649bd18
[tensorflow-serving]: https://tensorflow.github.io/serving/
[keras-tensorflow]: https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html
