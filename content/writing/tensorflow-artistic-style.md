---
date: 2016-09-30T21:00:00+01:00
title: Convolutional neural networks for artistic style transfer
category: machine-learning
tags:
   - image-processing
   - convolutional-neural-networks
   - keras
   - tensorflow
includes_code: yes
includes_math: yes
---

There's an amazing app out right now called [Prisma][prisma] that
transforms your photos into works of art using the styles of famous
artwork and motifs. The app performs this style transfer with the help
of a branch of machine learning called [convolutional neural
networks][cnn-wikipedia]. In this article we're going to take a
journey through the world of convolutional neural networks from theory
to practice, as we systematically reproduce Prisma's core visual
effect as a [webapp][neural-style-implementation].

## So what is Prisma and how might it work?

Prisma is a mobile app (iPhone and Android at the time this piece was
written) that allows you to transfer the style of one image, say an
impressionist painting, onto the content of another, say a picture of
your baby. Here's a demo of the kind of images that it generates.

{{< figure src="//placehold.it/1440x960/f4bc87/ffffff" title="TODO: A three part image, showing the content image, the style image and the style-transferred image generated by Prisma." >}}

Like many people, I find much of the output of this app very pleasing,
and I got curious as to how it achieves its visual effect. At the
outset you can imagine that it's somehow extracting low level features
like the colour and texture of the brush strokes from one image (that
we'll call the *style image*) and applying it to more semantic, higher
level features like a baby's face on the other image (the *content
image*).

How would one even begin to do something like this? You could perhaps
do some pixel-level image analysis on the style image to get things
like spatially-averaged colours or maybe even aspects of its texture,
but how would you then *apply* these in a selective fashion that still
retains the essential aspects of the content image? And what about the
existing style of the content image?  How do we first *discard* this
before we apply the new style?

I was stumped by many of these questions really early on, and as one
does, turned to Google for help. My searches soon pointed me to a
really popular paper ([Gatys et al., 2015][neural-style-gatys-etal])
that explains exactly how all this is achieved. In particular, the
paper poses what we're trying to do in mathematical terms as an
*optimisation problem*:

Let $\mathbf{c}$ be the content image and $\mathbf{s}$ be the style
image. We're trying to generate an image $\mathbf{x}$ that minimises
the following *loss function*:

$$
\mathcal{L}(\mathbf{c}, \mathbf{s}, \mathbf{x}) =
\alpha \mathcal{L}_c(\mathbf{c}, \mathbf{x}) +
\beta \mathcal{L}_s(\mathbf{s}, \mathbf{x})
$$

where $\mathcal{L}_c(\mathbf{c}, \mathbf{x})$ is the *content loss* (a
function that grows as the generated image $\mathbf{x}$ "deviates in
content" from $\mathbf{c}$), and $\mathcal{L}_s(\mathbf{s},
\mathbf{x})$ is the *style loss* (a function that grows when the
generated image $\mathbf{x}$ "deviates in style" from
$\mathbf{s}$). $\alpha$ and $\beta$ are scalar weighting factors that
control how much we want to emphasise the content relative to the
style.

One crucial bit of insight in this paper is that the definitions of
these style and content losses *are not based on per-pixel
differences* between images, but instead in terms of higher level,
more *perceptual differences* between them. But then how does one go
about writing a program that understands enough about the *meaning* of
images to do something like this?

This question leads to the next crucial bit of insight.  *It's nearly
impossible to solve a general problem like this well with an a priori
fixed set of rules.* So what we instead need is an abstract learning
machine that, when fed with a bunch of raw example data, can
automatically discover the representations needed to solve arbitrary
problems. As luck would have it, a class of abstract learning machines
that are particularly well suited to dealing with images already
exist, and they're called *Convolutional Neural Networks* (CNNs).

Over the course of this article, we're going to learn a ton. For those
who are new to all of this, we begin with a [deep dive into the world
of CNNs][cnn-primer]. We then learn how to use [CNNs to solve the
problem posed by Gatys et al.][neural-style-algorithm] and reproduce
the visual effect of Prisma. As a bonus, we conclude with a [concrete
implementation of the solution][neural-style-implementation] (in Keras
and TensorFlow, and wrapped in a Django webapp!) that you can play
with and extend.

## Convolutional Neural Networks from the ground up

This section offers a brief summary of parts of the Stanford course
[Convolutional Neural Networks for Visual Recognition
(CS231n)][cs231n] that are relevant to our style transfer problem. If
you’re even vaguely interested in what you're reading here, you should
probably take this course. *It is outstanding*.

### The image classification problem

We begin our journey with a look at the *image classification*
problem. This is a deceptively simple problem to state: Given an input
image, have a computer automatically classify it into one of a fixed
set of categories, say "baby", "dog", "car" or "toothbrush". The
reason we're starting with this problem is that it's at the core of
many seemingly unrelated tasks in computer vision, including our quest
to reproduce Prisma's visual effect.

In more precise terms, imagine a three channel colour image (RGB)
that's $W$ pixels wide and $H$ pixels tall. This image can be
represented in a computer as an array of $W \times H \times 3$
integers, each going between $0$ (minimum brightness) and $255$
(maximum brightness). Let's further assume that we have $K$ categories
of things that we'd like to classify the image as being one of. The
task then is to come up with a function that takes as input one of
these large arrays of numbers, and outputs the correct label from our
set of categories, e.g. "baby".

{{< figure src="//placehold.it/1440x960/f4bc87/ffffff" title="TODO: The image classification problem." >}}

How might we write such a classification function? One naïve approach
would be to hardcode some characteristics of babies (such as large
heads, snotty noses, rounded cheeks, ...) into our function. But even
if you knew how to do this, what if you then wanted to look for cars?
What about different kinds of cars? What about worms? What if our set
of $K$ categories became arbitrarily large and nuanced?

To further complicate the problem, note that any slight change in the
situation under which the image was captured (illumination, viewpoint,
background clutter, ...) greatly affects the array of integers being
passed as input to our function. How do we write our classification
function to ignore these sorts of superfluous differences while still
giving it the ability to distinguish between a "baby" and a "small
child"?

Since this is starting to look hopeless on multiple fronts, we turn to
a completely different approach --- one that's more *data driven*. We
first gather a bunch of pre-classified images as examples and then
feed them into a *learning algorithm*. This algorithm uses the
examples to learn about the visual appearance of each class, allowing
it to automatically function as the classifier we want!

While this does sound rather amazing, it's also very hand-wavy. Let's
make things more concrete by taking a look at one of the simplest
learning image classifiers: A [*Softmax classifier* with a
*cross-entropy* loss][cs231n-softmax-classifier] function.

### A first learning image classifier

#### A linear score function

Recall the classification problem we're trying to solve. We have an
image $\mathbf{x}$ that's represented as an array of integers of
length $D = W \times H \times 3$, and we want to find out which
category (in a set of $K$ categories) it belongs to. In fact, instead
of just reporting one category name, it would be more helpful to get a
*confidence score* for each category. This way, we'll not only get the
primary category we're looking for (the largest score), but we'll also
have a sense of how confident we are with our classification.

So in essence, what we're looking for is a *score* function $f:
\mathbb{R}^D \mapsto \mathbb{R}^{K}$ that maps image data to class
scores. The simplest possible example of such a function is a linear
map:

$$
f(\mathbf{x}; \mathbf{W}, \mathbf{b}) = \mathbf{W}\mathbf{x} + \mathbf{b}
$$

Here, the matrix $\mathbf{W}$ (of size $K \times D$) and the vector
$\mathbf{b}$ (of size $K \times 1$) are *parameters* of the
function. The algorithm will *learn* these with the help of our
pre-classified examples. And once we've learnt (fit) the parameters on
this *training data*, we hopefully have a function that *generalises*
well enough to classify arbitrary image input.

#### Softmax activation and cross entropy loss

The first step in the learning process is to introduce a *loss*
function, $\mathcal{L}$. This is a function that quantifies the
*disagreement* between what our classifier suggests for the scores and
what our training data provides as the known truth. Thus, this loss
function goes up if the classifier is doing a poor job and goes down
if it's doing great. And the goal of the learning process is determine
parameters that give us the best (lowest) loss.

{{< figure src="//placehold.it/1440x960/f4bc87/ffffff" title="TODO: An image classifier showing the score function and the loss function" >}}

Suppose our training data is a set of $N$ pre-classified examples
$\mathbf{x_i} \in \mathbb{R}^D$, each with correct category $y_i \in
1, \ldots, K$. A good functional form to determine the loss for one of
these examples is:

$$
\mathcal{L}\_{\mathrm{data}\_i} =
-\log\left(\frac{\exp(f\_{y\_i}(\mathbf{x}_i))}
{\sum\_{j=1}^K\exp(f_j(\mathbf{x}_i))}\right)
$$

where $f_j(\mathbf{x}_i)$ is the $j$<sup>th</sup> element of the
vector $f(\mathbf{x}_i)$. This is called the [cross
entropy][cross-entropy] loss of the [softmax][softmax] of the class
scores determined by $f$. As weird as this form looks, if you stare at
it long enough you'll convince yourself of a few things:

1. The stuff in parenthesis takes the output of $f(\mathbf{x}_i)$,
which is a vector of $K$ real values, plucks the value at the correct
class' position ($y_i$), and transforms it into a single number in the
range $(0, 1)$. This allows us to interpret this output as the
probability our score function believes $y_i$ is the correct class.

2. The negative $\log$ of $(0, 1) \mapsto (\infty, 0)$. Meaning that
if our score function identifies the correct answer with high
probability, the loss function tends to $0$. And if it identifies the
correct answer with low probability, the loss function tends to
$\infty$.

3. This form is smoothly differentiable relative to our parameters
$(\mathbf{W}, \mathbf{b})$. We'll soon see why this is a useful
property to have.

To go from the loss on a single training example to the entire set, we
simply average over all our examples:

$$
\mathcal{L}\_\mathrm{data} = \frac{1}{N}\sum_{i=1}^N
\mathcal{L}\_{\mathrm{data}\_i}
$$

TODO: Regularisation term.

#### An iterative optimisation process

Now that we have a loss function that measures the quality of our
classification, all we have left to do is to find parameters that
minimise this loss. This is an optimisation problem.

There are a lot of bad ways to solve this problem (e.g. guessing
parameters until we get lucky), but one good way to solve this
problem is by *iterative refinement*. This is where we start with
random values for our parameters $(\mathbf{W}, \mathbf{b})$, and
successively improve them step-by-step until the loss is
minimised.

If you imagine the loss function to be a bowl-like surface (albeit in
multiple dimensions), what we're trying to do is to find the lowest
point in this bowl. How would you do this if you couldn't see the
entire bowl? You'd start somewhere and feel around in your local
neighbourhood, and move toward whatever direction you find the
steepest downward slope. You stop when you can't go any lower (or the
slope goes to 0). The technical term for this approach is called
[gradient descent][gradient-descent]. (In fact, there is a [whole
family of related methods][gradient-descent-family] that improve on
this basic idea, but we'll start with the basic version first.)

TODO: Describe the math behind (minibatch) SGD; decay learning rate
over the period of the training

*Finally*, we have our first complete learning image classifier. Given
some image as a raw array of numbers, we have a parameterised (score)
function that takes us to category scores. We have a way of evaluating
its performance (the loss function). We also have an algorithm to
learn and improve the classifier's parameters with example data
(optimisation via stochastic gradient descent).

We have quite a bit more theory to go before we understand all the
bits we need to [solve Gatys et al.'s optimisation
problem][neural-style-algorithm] and reproduce Prisma's visual
effect. But if you're itching for some practical exercises, now is a
good time to pause reading and try the [first TensorFlow
tutorial][tensorflow-tutorial-mnist] aimed at beginners to machine
learning. You now have enough background to appreciate the choicen
they've made in the tutorial.

### Moving to neural networks

The linear image classifier we just introduced doesn't work very well
in general. What it's attempting to do is to draw a bunch of lines
($n-1$ dimensional hyperplanes, really) in a plane ($n$ dimensional
space, really) of images, hoping to carve it out into categories. And
if you think about it, you'll see that this approach can only succeed
if the image data we're working with is conveniently linearly
separable in our chosen space of images.

{{< figure src="//placehold.it/1440x960/f4bc87/ffffff" title="TODO: Cartoon representation of the image space as a 2D plane, with the classifier being a bunch of lines." >}}

Even so, the reason we spent *so much time* on this first image
classifier is that it was a way to introduce the parameterised score
function, the loss function, and the iterative optimisation process,
all without being bogged down by too many other technical details.
Now that we understand what these are and how they allow us to build a
learning image classifier, we will soon extend the score function to
more complex, nonlinear forms. We'll begin first with neural networks
in general, and then move on to convolutional neural networks. But the
rest of the ideas (loss function, optimisation process) stay the same.

#### A first nonlinearity

TODO: Introduce ReLU as a first nonlinear extension, serving as our
first model of a *neuron*. There are many other [functional
forms][todo] one could use, but this one form is really popular today
and will suffice for our needs.

#### Layer-wise organisation into a network

TODO: Talk about organising collections of neurons into (acyclic)
graphs. This introduces the fully-connected (FC) layer. More layers
allow for more nonlinearity, even though each neuron is barely
nonlinear.

{{< figure src="/images/writing/tensorflow-artistic-style/neural-network.svg" title="TODO: An example neural network image." >}}

TODO: Note that this allows for a now classic architecture that
employs matrix multiplications interwoven with nonlinear *activation*
functions.

#### Some technicalities

TODO: Introduce batchnorm(?) and regularisation to prevent
over-fitting of such dense networks.

TODO: Offer some conclusions on NNs in general and setup a simple
exercise in TensorFlow. The point is to try to improve upon the linear
image classifier we had earlier, and motivate Keras as a means to
eliminate boilerplate code.

### And finally, convolutional neural networks

- TODO: FC NNs quickly balloon to too many parameters. Turns out we
  can get much better performance with far fewer parameters by
  exploiting the fact that we have a 2D image.

#### Architecture of CNNs in general

- TODO: Introduce *Convolutional Layers*, *Pooling Layers* and recall
  Fully-Connected Layers and Softmax. Parameter sharing greatly
  reduces the number of parameters we're dealing with.
- TODO: Convolutional Neural Networks consist of layers of small
  computational units that process visual information hierarchically
  in a *feed-forward* manner. Each layer of units can be
  understood as a collection of image filters, each of which extracts
  a certain feature from the input image. Thus, the output of a given
  layer consists of so-called feature maps: differently filtered
  versions of the input image.

#### A powerful CNN image classifier

- TODO: Summarise the Simonyan, et al. paper (VGG19)
- TODO: Note that they've shared their learnt weights, so we can
  *transfer* this knowledge over for our purposes.
- TODO: Setup an exercise to duplicate this classifier in Keras. Turns
  out this can be done trivially in recent Keras, and this is what
  we're going to employ henceforth.

## Returning to the style transfer problem

If you've made it this far, you're probably starting to realise that
the whole *quest to reproduce Prisma's visual effect* was simply a
ruse to get you to trudge through all this background on neural
networks. But congratulations, you're now not only ready to solve this
original style transfer problem, you're also in a position to read,
understand and reproduce state-of-the-art research articles in the
field of convolutional neural networks! This is a pretty big deal, so
spend some time celebrating this fact. And when you're done, let's
return to the style transfer problem.

### A neural algorithm of artistic style

- TODO: Summarise the Gatys, et al. paper for the core ideas (and a
  sketch of the solution methodology):
  - CNNs pre-trained for image classification (in particular the VGG
  introduced above) have already learnt to encode perceptual and
  semantic information that we need to measure our losses. The
  explanation could be that when learning object recognition, the
  network has to become invariant to all image variation that
  preserves object identity.
  - Higher layers in the network capture the high-level content in
  terms of objects and their arrangement in the input image but do not
  constrain the exact pixel values of the reconstruction. To obtain a
  representation of the style of an input image, we employ
  correlations between the different filter responses over the spatial
  extent of the feature maps.
  - The representations of style and content in CNNs are separable.
  - The images are synthesised by finding an image that simultaneously
  matches the content representation of the photograph and the style
  representation of the respective piece of art.
- TODO: Recall the Gatys problem, which now seems a lot less
  intimidating. We're going to simply reuse a trained VGG to solve
  it.
  - TODO: Based on VGG19 - 3 FC layers. Normal VGG takes an image and
  returns a category score, but Gatys instead take the outputs at
  intermediate layers and construct L_content and L_style.
  - TODO: A figure showing off the algorithm.
  - TODO: Introduce L-BFGS as a valid quasi-Newton approach to solve
  the optimisation problem.

### Concrete implementation of the Gatys optimisation problem

- TODO: Since this is a very popular paper, there are many existing
  implementations of it online. Point to two of the better ones on
  GitHub. Step through some crucial portions of these.

- TODO: Talk about how hyperparameters are tuned to improve aesthetic
  quality of the output. Show examples of things that work and things
  that do not.

- TODO: Conclude with an exercise to re-implement this in Keras. Now
  relatively easy to do since VGG itself is trivial to reproduce.

## Incorporating this model into a webapp

- TODO: Explain the general architecture of the webapp, with a
  figure.

- TODO: Point out that solving the entire optimisation problem is not
  feasible as part of the request-response cycle.

### A faster approach to neural style transfer

- TODO: Introduce the Johnson paper and discuss its core idea:
  Replacing the optimisation problem with an *image transformation*
  CNN, which in turn uses the VGG as before to measure losses. When
  this transformation network is trained on many images given a fixed
  style image, we end up with a fully feed-forward CNN that we can
  apply for style transfer. This gives us a 1000x speed up over Gatys'
  implementation, which makes it suitable for a webapp.

- TODO: Architecture notes, introducing a new residual block.

| Layer                       | Activation Size |
| ----------------------------|---------------- |
| Input                       | 3 × 256 × 256   |
| 32×9×9 conv, stride 1       | 32 × 256 × 256  |
| 64×3×3 conv, stride 2       | ...             |
| 128×3×3 conv, stride 2      |                 |
| Residual block, 128 filters |                 |
| ...                         |                 |


- TODO: A figure showing off the algorithm, and how it differs from
  Gatys.

- TODO: Point to our (Keras) implementation of this paper on
  GitHub. This is one significant contribution of this work.

### Serving a learnt model

- TODO: Explain the theory behind exporting a learnt model from Keras
  (as TensorFlow data structures?) and serving it (with TensorFlow
  serving?)
- TODO: Step through important aspects of the implementation of the
  Django webapp. Key models, using generic views.
- TODO: Point to the project source for the complete webapp.

## Conclusion

- TODO: Show many beautiful examples for our program in action, and
  point to it online. Refer back to first motivating examples from
  Prisma.
- TODO: Reiterate some insights.
  - Maybe a giant array of pixels is not the best way of representing
    an image if we wish to understand it better.
  - Some classes of problems are hard to solve with a priori known
    rules, and require learning machines. -> Have them automatically
    discover the representations needed to solve arbitrary problems.
- TODO: Point out that you're now ready to do much more than just this
  problem.
- TODO: Talk about ideas for extension and improvement.

## Selected references and further reading

1. [A Neural Algorithm of Artistic Style][neural-style-gatys-etal],
   the seminal article
2. [Very Deep Convolutional Networks for Large-Scale Image
   Recognition][vgg-simonyan-etal]
3. [Perceptual Real-Time Style
   Transfer][fast-neural-style-johnson-etal] and [supplementary
   material][fast-neural-style-johnson-etal-supp]
4. [Deep learning][deep-learning-review], a review in Nature
5. [Calculus on Computational Graphs: Backpropagation][backprop-explanation]
6. [The Stanford course on Convolutional Neural Networks][cs231n] and
   [accompanying notes][cs231n-notes]
7. [Our sample implementation on GitHub][neural-style-demo-project]
8. TensorFlow: [Deep CNNs][tensorflow-cnn], [GPU support on
   macOS][tensorflow-gpu-macos], [Serving][tensorflow-serving]
9. [Keras as a simplified interface to TensorFlow][keras-tensorflow]

[cnn-primer]: #convolutional-neural-networks-from-the-ground-up
[neural-style-implementation]: #incorporating-this-model-into-a-webapp
[neural-style-algorithm]: #returning-to-the-style-transfer-problem
[neural-style-demo-project]: https://github.com/hnarayanan/stylist
[prisma]: http://prisma-ai.com
[cnn-wikipedia]: https://en.wikipedia.org/wiki/Convolutional_neural_network
[neural-style-gatys-etal]: https://arxiv.org/abs/1508.06576
[fast-neural-style-johnson-etal]: https://arxiv.org/abs/1603.08155
[fast-neural-style-johnson-etal-supp]: https://cs.stanford.edu/people/jcjohns/papers/fast-style/fast-style-supp.pdf
[vgg-simonyan-etal]: https://arxiv.org/abs/1409.1556
[deep-learning-review]: https://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf
[backprop-explanation]: http://colah.github.io/posts/2015-08-Backprop/
[cs231n]: http://cs231n.stanford.edu
[cs231n-notes]: http://cs231n.github.io
[cs231n-softmax-classifier]: http://cs231n.github.io/linear-classify/#softmax-classifier
[tensorflow-cnn]: https://www.tensorflow.org/versions/r0.10/tutorials/deep_cnn/index.html
[tensorflow-gpu-macos]: https://gist.github.com/ageitgey/819a51afa4613649bd18
[tensorflow-serving]: https://tensorflow.github.io/serving/
[tensorflow-tutorial-mnist]: https://www.tensorflow.org/versions/r0.10/tutorials/mnist/beginners/index.html
[keras-tensorflow]: https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html
[cross-entropy]: https://en.wikipedia.org/wiki/Cross_entropy
[softmax]: https://en.wikipedia.org/wiki/Softmax_function
[gradient-descent]: https://en.wikipedia.org/wiki/Gradient_descent
[gradient-descent-family]: http://cs231n.github.io/neural-networks-3/#update
[todo]: todo
