---
date: 2016-10-15T21:00:00+01:00
title: Convolutional neural networks for artistic style transfer
category: machine-learning
tags:
   - image-processing
   - convolutional-neural-networks
   - keras
   - tensorflow
includes_code: yes
includes_math: yes
---

There's an amazing app out right now called [Prisma][prisma] that
transforms your photos into works of art using the styles of famous
artwork and motifs. The app performs this style transfer with the help
of a branch of machine learning called [convolutional neural
networks][cnn-wikipedia]. In this article we're going to take a
journey through the world of convolutional neural networks from theory
to practice, as we systematically reproduce Prisma's core visual
effect.

## So what is Prisma and how might it work?

Prisma is a mobile app (iPhone and Android at the time this piece was
written) that allows you to transfer the style of one image, say an
impressionist painting, onto the content of another, say a picture of
your baby. Here's a demo of the kind of images that it generates.

{{< figure src="/images/projects/placeholder.svg" title="TODO: A three part image, showing the content image, the style image and the style-transferred image generated by Prisma." >}}

Like many people, I find much of the output of this app very pleasing,
and I got curious as to how it achieves its visual effect. At the
outset you can imagine that it's somehow extracting low level features
like the colour and texture from one image (that we'll call the *style
image*) and applying it to more semantic, higher level features like a
baby's face on the other image (the *content image*).

How would one even begin to achieve something like this? You could
perhaps do some pixel-level image analysis on the style image to get
things like spatially-averaged colours or maybe even aspects of its
texture, but how would you then *apply* these in a selective fashion
that still retains the essential aspects of the content image? And
what about the existing style of the content image?  How do we first
*discard* this before we apply the new style?

I was stumped by many of these questions really early on, and as one
does, turned to Google for help. My searches soon pointed me to a
really popular paper ([Gatys et al., 2015][neural-style-gatys-etal])
that explains exactly how all this is achieved. In particular, the
paper poses what we're trying to do in mathematical terms as the
following *optimisation problem*:

Let $\mathbf{c}$ be the content image and $\mathbf{s}$ be the style
image. We're trying to generate an image $\mathbf{x}$ that minimises
the following *loss function*:

$$
\mathcal{L}(\mathbf{c}, \mathbf{s}, \mathbf{x}) =
\alpha \mathcal{L}_c(\mathbf{c}, \mathbf{x}) +
\beta \mathcal{L}_s(\mathbf{s}, \mathbf{x})
$$

where $\mathcal{L}_c(\mathbf{c}, \mathbf{x})$ is the *content loss* (a
function that grows as the generated image $\mathbf{x}$ "deviates in
content" from $\mathbf{c}$), and $\mathcal{L}_s(\mathbf{s},
\mathbf{x})$ is the *style loss* (a function that grows when the
generated image $\mathbf{x}$ "deviates in style" from
$\mathbf{s}$). $\alpha$ and $\beta$ are scalar weighting factors that
control how much we want to emphasise the content relative to the
style.

One crucial bit of insight in this paper is that the definitions of
these style and content losses *are not based on per-pixel
differences* between images, but instead in terms of higher level,
more *perceptual differences* between them. But then how does one go
about writing a program that understands enough about the *meaning* of
images to do something like this?

This question leads to the next crucial bit of insight.  *It's nearly
impossible to solve a general problem like this well with an a priori
fixed set of rules.* So what we instead need is an abstract learning
machine that, when fed with a bunch of raw example data, can
automatically discover the representations needed to solve arbitrary
problems. As luck would have it, a class of abstract learning machines
that are particularly well suited to dealing with images already
exist, and they're called *Convolutional Neural Networks* (CNNs).

Over the course of this article, we're going to learn a ton. For those
who are new to all of this, we begin with a [deep dive into the world
of CNNs][cnn-primer]. We then learn how to use [CNNs to solve the
problem posed by Gatys et al.][neural-style-algorithm] and reproduce
the visual effect of Prisma. As a bonus, we conclude with a [concrete
implementation of the solution][neural-style-implementation] (in Keras
and TensorFlow) that you can play with and extend.

## Convolutional Neural Networks from the ground up

This section offers a brief summary of parts of the Stanford course
[Convolutional Neural Networks for Visual Recognition
(CS231n)][cs231n] that are relevant to our style transfer problem. If
you’re even vaguely interested in what you're reading here, you should
probably take this course. *It is outstanding*.

### The image classification problem

We begin our journey with a look at the *image classification*
problem. This is a deceptively simple problem to state: Given an input
image, have a computer automatically classify it into one of a fixed
set of categories, say "baby", "dog", "car" or "toothbrush". And the
reason we're starting with this problem is that it's at the core of
many seemingly unrelated tasks in computer vision, including our quest
to reproduce Prisma's visual effect.

In more precise terms, imagine a three channel colour image (RGB)
that's $W$ pixels wide and $H$ pixels tall. This image can be
represented in a computer as an array of $W \times H \times 3$
integers, each going between $0$ (minimum brightness) and $255$
(maximum brightness). Let's further assume that we have $K$ categories
of things that we'd like to classify the image as being one of. The
task then is to come up with a function that takes as input one of
these large arrays of numbers, and outputs the correct label from our
set of categories, e.g. "baby".

{{< figure src="/images/projects/placeholder.svg" title="TODO: The image classification problem." >}}

How might we write such a classification function? One naïve approach
would be to hardcode some characteristics of babies (such as large
heads, snotty noses, rounded cheeks, ...) into our function. But even
if you knew how to do this, what if you then wanted to look for cars?
What about different kinds of cars? What about worms? What if our set
of $K$ categories became arbitrarily large and nuanced?

To further complicate the problem, note that any slight change in the
situation under which the image was captured (illumination, viewpoint,
background clutter, ...) greatly affects the array of integers being
passed as input to our function. How do we write our classification
function to ignore these sorts of superfluous differences while still
giving it the ability to distinguish between a "baby" and a "small
child"?

Since this is starting to look hopeless on multiple fronts, we turn to
a completely different approach --- one that's more *data driven*. We
first gather a bunch of pre-classified images as examples and then
feed them into a *learning algorithm*. This algorithm uses the
examples to learn about the visual appearance of each class, allowing
it to automatically function as the classifier we want!

While this does sound rather amazing, it's also very hand-wavy. Let's
start to make things more concrete by taking a look at one of the
simplest learning image classifiers: A [*Softmax classifier* with a
*cross-entropy* loss][cs231n-softmax-classifier] function.

### Our first learning image classifier

#### A linear score function

Recall the classification problem that we're trying to solve. We have
an image $\mathbf{x}$ that's represented as an array of integers of
length $D = W \times H \times 3$, and we want to find out which
category (in a set of $K$ categories) that it belongs to. In fact,
instead of just reporting one category name, it would be more helpful
to get a *confidence score* for each category. This way, we'll not
only get the primary category we're looking for (the largest score),
but we'll also have a sense of how confident we are with our
classification.

So in essence, what we're looking for is a *score* function $f:
\mathbb{R}^D \mapsto \mathbb{R}^{K}$ that maps image data to class
scores. The simplest possible example of such a function is a linear
map:

$$
f(\mathbf{x}; \mathbf{W}, \mathbf{b}) = \mathbf{W}\mathbf{x} + \mathbf{b}
$$

Here, the matrix $\mathbf{W}$ (of size $K \times D$) and the vector
$\mathbf{b}$ (of size $K \times 1$) are *parameters* of the
function. The algorithm will *learn* these with the help of our
pre-classified examples. And once we've learnt (fit) the parameters on
this *training data*, we hopefully have a function that *generalises*
well enough to classify arbitrary image input (called *test data*).

#### Softmax activation and cross entropy loss

The first step in the learning process is to introduce a *loss*
function, $\mathcal{L}$. This is a function that *quantifies the
disagreement* between what our classifier suggests for the scores and
what our training data provides as the known truth. Thus, this loss
function goes up if the classifier is doing a poor job and goes down
if it's doing great. And the goal of the learning process is determine
parameters that give us the best (lowest) loss.

{{< figure src="/images/projects/placeholder.svg" title="TODO: An image classifier showing the score function and the loss function" >}}

Suppose our training data is a set of $N$ pre-classified examples
$\mathbf{x_i} \in \mathbb{R}^D$, each with correct category $y_i \in
1, \ldots, K$. A [good functional form][cross-entropy-reason] to
determine the loss for one of these examples is:

$$
\mathcal{L}\_{\mathrm{data}\_i} =
-\log\left(\frac{\exp(f\_{y\_i}(\mathbf{x}_i))}
{\sum\_{j=1}^K\exp(f_j(\mathbf{x}_i))}\right)
$$

where $f_j(\mathbf{x}_i)$ is the $j$<sup>th</sup> element of the
vector $f(\mathbf{x}_i)$. This is called the [cross
entropy][cross-entropy] loss of the [softmax][softmax] of the class
scores determined by $f$. As weird as this form looks, if you stare at
it long enough you'll convince yourself of a few things:

1. The stuff in the big parenthesis takes the output of
$f(\mathbf{x}_i)$, which is a vector of $K$ real values, plucks the
value at the correct class' position ($y_i$), and transforms it into a
single number in the range $(0, 1)$. This allows us to interpret this
output as the probability our score function believes $y_i$ is the
correct class.

2. The negative $\log$ of $(0, 1) \mapsto (\infty, 0)$. Meaning that
if our score function identifies the correct answer with high
probability, the loss function tends to $0$. And if it identifies the
correct answer with low probability, the loss function tends to
$\infty$.

3. This form is smoothly differentiable relative to our parameters
$(\mathbf{W}, \mathbf{b})$. We'll soon see why this is a useful
property to have.

To go from the loss on a single training example to the entire set, we
simply average over all our $N$ examples:

$$
\mathcal{L}\_\mathrm{data} = \frac{1}{N}\sum_{i=1}^N
\mathcal{L}\_{\mathrm{data}\_i}
$$

TODO: Note here that the optimisation problem is not well posed, so we
need a *regularisation term* to constrain the parameters search
space.

TODO: Conclude with the full loss function.

#### An iterative optimisation process

Now that we have a loss function that measures the quality of our
classification, all we have left to do is to find parameters that
minimise this loss. This is an optimisation problem.

There are a lot of bad ways to solve this problem (e.g. guessing
parameters until we get lucky), but one good way to solve this
problem is by *iterative refinement*. This is where we start with
random values for our parameters $(\mathbf{W}, \mathbf{b})$, and
successively improve them step-by-step until the loss is
minimised.

If you imagine the loss function to be a bowl-like surface (albeit in
multiple dimensions), what we're trying to do is to find the lowest
point in this bowl. How would you do this if you couldn't see the
entire bowl? You'd start somewhere and feel around in your local
neighbourhood, and move toward whatever direction you find the
steepest downward slope. You stop when you can't go any lower (or the
slope goes to 0). The technical term for this approach is called
[gradient descent][gradient-descent]. (In fact, there is a [whole
family of related methods][gradient-descent-family] that improve on
this basic idea, but we'll start with the basic version first.)

TODO: An explanatory figure goes here.

TODO: Describe the math behind (minibatch) SGD; decay learning rate
over the period of the training

TODO: Need to introduce L-BFGS at some point since this is the
optimisation algorithm used in Gatys et al.

---

Finally we have our first complete learning image classifier! Given
some image as a raw array of numbers, we have a parameterised (score)
function that takes us to category scores. We have a way of evaluating
its performance (the loss function). We also have an algorithm to
learn and improve the classifier's parameters with example data
(optimisation via stochastic gradient descent).

We have quite a bit more theory to go before we understand all the
bits we need to [solve Gatys et al.'s optimisation
problem][neural-style-algorithm] and reproduce Prisma's visual
effect. But now is a good time to pause on theory and work through
your first exercise: [the TensorFlow MNIST classification
tutorial][tensorflow-tutorial-mnist] aimed at beginners to machine
learning. Working through this tutorial will ensure that you have
TensorFlow properly running on your machine, and allows you to
experience coding up an image classifier to see all the pieces we
talked about in action. The background material we've covered will
allow you to appreciate the choices they've made in the tutorial.

Have fun practising, and I'll see you when you're done!

### Moving to neural networks

The linear image classifier you just built following the tutorial
above works surprisingly well for the [MNIST digit
dataset][mnist-dataset] (around 92% accurate). But if you attempted to
extend the tutorial to more general images, you'd have realised that
it performs rather poorly. This is because what the linear classifier
is attempting to do is to draw a bunch of lines ($n-1$ dimensional
hyperplanes, really) in a plane ($n$ dimensional space, really) of
images, hoping to carve it out into categories. And if you think about
it, you'll see that this approach can only succeed if the image data
we're working with is conveniently linearly separable in our chosen
space of images. (Somewhat true for the MNIST dataset, and not at all
true in general.)

{{< figure src="/images/projects/placeholder.svg" title="TODO: Cartoon representation of the image space as a 2D plane, with the classifier being a bunch of lines." >}}

Even so, the reason we spent so much time on the linear image
classifier is that it was a way to introduce the *parameterised score
function*, the *loss function*, and the *iterative optimisation
process*, all without being bogged down by too many other technical
details.  Now that we understand what these are and how they work
together to build a learning image classifier, we are going to improve
the performance of the classifier by extending the score function to
more complex (nonlinear) forms. The first of these extensions will be
to (fully-connected) [neural networks][todo], and we'll then move on
to [convolutional neural networks][todo].

The cool thing is that as we're working through these generalisations
of the score function, the rest of the ideas (the loss function and
optimisation process) stay the same!

#### Making the score function nonlinear

The score function we started this story with was the simplest
possible we could imagine:

$$f(\mathbf{x}; \mathbf{W}, \mathbf{b}) =
\mathbf{W}\mathbf{x} + \mathbf{b}$$

TODO: Introduce bias trick much earlier

$$f(\mathbf{x}; \mathbf{W}) =
\mathbf{W}\mathbf{x}$$

To extend this to a nonlinear regime, we're going to pass the output
of this function (elementwise) through a simple nonlinear function
called the *rectified linear unit* (or *ReLU*) $g(x) = \max(0, x)$.

TODO: Figure of the ReLU

$$
f(x; W_1, W_2) = W_2 \max(0, W_1 x)
$$

To increase the nonlinearity of the score function, we repeat this
process, e.g.

$$
f(x; W_1, W_2, W_3) = W_3 \max(0, W_2 \max(0, W_1 x))
$$

TODO: Introduce ReLU as a first nonlinear extension, serving as our
first model of a *neuron*. There are many other [functional
forms][activation-functions] one could use, but this one form is really popular today
and will suffice for our needs.

#### Layer-wise organisation into a network

TODO: Talk about organising collections of neurons into (acyclic)
graphs. This introduces the fully-connected (FC) layer. More layers
allow for more nonlinearity, even though each neuron is barely
nonlinear.

{{< figure src="/images/writing/artistic-style-transfer/neural-network.svg" title="TODO: An example neural network image." >}}

TODO: Note that this allows for a now classic architecture that
employs matrix multiplications interwoven with nonlinear *activation*
functions.

#### Some technicalities

TODO: Introduce batchnorm(?) and regularisation to prevent
over-fitting of such dense networks.

TODO: Offer some conclusions on NNs in general and setup a simple
exercise in TensorFlow. The point is to try to improve upon the linear
image classifier we had earlier, and motivate Keras as a means to
eliminate boilerplate
code. e.g. [1](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3_NeuralNetworks/multilayer_perceptron.ipynb),
[2](http://cs231n.github.io/neural-networks-case-study/),
[3](https://www.youtube.com/watch?v=lTFOw8-P02Y)?

You now know enough to extend the one step MNIST tensorflow tutorial
into multi-layer and try it out. Note that your accuracy on MNIST goes
from ca 92 to 97.

### And finally, convolutional neural networks

We're in a really good place right now in terms of our understanding
and capability. We've managed to build a (two-layer) neural network
that does an excellent job of classifying images. (Over 97% on the
MNIST data.) You would've realised through the exercise that this
extension didn't take too much more code than the linear classifier we
built in the first exercise.

If I were to ask you now how we could further improve the accuracy of
our classifier, you'd probably point out that this is easy to do by
adding more layers to our score function (making our model
*deeper*). This is indeed true, but if you were to take a step back
and look at our model, you'll see two problems:

1. We began the classification process by representing the image as an
array that's $W \times H \times 3$ long. By thinking of the input as a
line of data, we've already lost information about the structure of
the original image. (You can imagine that pixels that are nearby share
context.)

2. The number of parameters we would need to train quickly becomes
unwieldy as the input image dimensions or number of layers
grow. E.g. for our two-layer model, it is TODO. Extending this to a
three-layer model with TODO.

Convolutional neural networks (CNNs) are architected to solve both
these issues. And we'll soon see how we can use them to build a deep
image classifier that's state of the art.

#### Architecture of CNNs in general

In many ways, CNNs are much the same as the ordinary (fully-connected)
neural networks that we've seen so far. They're a collection neurons
arranged into layers that each perform a linear map followed
(potentially) by a nonlinear activation. As the input goes through
these layers, it is sequentially transformed into a representation
that makes it suitable to the problem at hand (image
classification). As before, the whole network still represents a
single score function that's passed to a loss function to evaluate how
well it's doing.

The things that make CNNs special are:

1. Instead of dealing with the input data (and arranging intermediate
layers of neurons) as lines, they arrange neurons in a 3D fashion
(width, height, depth).

2. Neurons in one layer only connect to a small portion of the
previous layer (as opposed to *every* neuron in the previous layer).

3. Neurons in a given layer *share their weights*.

Such an architecture allows them to retain a lot of the structure
that's inherent to image data (the spatial arrangement of pixels, the
fact that pixels nearby share context) and prevents the number of
model parameters from growing too unwieldy, even as we introduce
additional layers. This makes them more efficient to implement and
greatly reduces memory requirements when compared to standard neural
networks.

TODO: Figure of the differences between standard and convolutional
neural networks.

In addition to the standard *fully connected* (FC) layer and the *ReLU
layer* we've already seen, CNNs are additionally made up of the
following kinds of layers. Each layer of units can be understood as a
collection of image filters, each of which extracts a certain feature
from the input image.

##### Convolutional (Conv) layer

TODO: Parameter sharing greatly reduces the number of parameters we're
dealing with.

##### Pooling (Pool) layer

TODO: Has no parameters or hyperparameters, simply reduce the
computational complexity of the problem at hand. Also reduces
over-fitting.

TODO: Recall that with this notation, the models we've seen so far
look like the following:
```
Linear: Input -> FC -> Loss
NN: Input -> FC -> ReLU -> FC -> Loss
```

TODO: A simple CNN-based image classifier for CIFAR10 goes here? Need
to shift to Keras at some point to reduce boilerplate code.

#### A powerful CNN-based image classifier

Now that we have the vocabulary to talk about CNNs in general, we turn
our attention to a specific CNN-based image classifier, one that
happens to be central to our original style transfer problem:
[VGGNet][vgg-simonyan-etal]. VGGNet was introduced as one of the
contenders in 2014's ImageNet Challenge and secured the first and the
second places in the localisation and classification tracks
respectively. It was later described in great detail in [a paper that
came out the following year][vgg-simonyan-etal]. The paper describes
how a family of models  essentially composed of simple ($3 \times 3$)
convolutional filters with increasing depth (11--19 layers) managed to
perform so well at a range of computer vision tasks.

{{< figure src="/images/projects/placeholder.svg" title="TODO: The architecture of the VGGNet family." >}}

Note that the last FC layer has 1000 neurons.

- TODO: Note that they've shared their learnt weights, so we can
  *transfer* this knowledge over for our purposes.
- TODO: Setup an exercise to duplicate this classifier in Keras. Turns
  out this can be done trivially in recent Keras, and this is what
  we're going to employ henceforth.

## Returning to the style transfer problem

If you've made it this far, you're probably starting to realise that
the whole *quest to reproduce Prisma's visual effect* was simply a
ruse to get you to trudge through all this background on neural
networks. But congratulations, you're now not only ready to solve this
original style transfer problem, you're also in a position to read,
understand and reproduce state-of-the-art research articles in the
field of convolutional neural networks! This is a pretty big deal, so
spend some time celebrating this fact. And when you're done, let's
return to the style transfer problem.

### A neural algorithm of artistic style

- TODO: Summarise the Gatys, et al. paper for the core ideas (and a
  sketch of the solution methodology):
  - CNNs pre-trained for image classification (in particular the VGG
  introduced above) have already learnt to encode perceptual and
  semantic information that we need to measure our losses. The
  explanation could be that when learning object recognition, the
  network has to become invariant to all image variation that
  preserves object identity.
  - Higher layers in the network capture the high-level content in
  terms of objects and their arrangement in the input image but do not
  constrain the exact pixel values of the reconstruction. To obtain a
  representation of the style of an input image, we employ
  correlations between the different filter responses over the spatial
  extent of the feature maps.
  - The representations of style and content in CNNs are separable.
  - The images are synthesised by finding an image that simultaneously
  matches the content representation of the photograph and the style
  representation of the respective piece of art.
- TODO: Recall the Gatys problem, which now seems a lot less
  intimidating. We're going to simply reuse a trained VGG to solve
  it.
  - TODO: Based on VGG19 - 3 FC layers. Normal VGG takes an image and
  returns a category score, but Gatys instead take the outputs at
  intermediate layers and construct L_content and L_style.
  - TODO: A figure showing off the algorithm.
  - TODO: Introduce L-BFGS as a valid quasi-Newton approach to solve
  the optimisation problem.

### Concrete implementation of the artistic style transfer algorithm

Since Gatys et al. is a very exciting paper, there exist many
open source implementations of the algorithm online. One of the most
popular and general purpose ones is by [Justin Johnson and implemented
in Torch](https://github.com/jcjohnson/neural-style). I've instead
followed a [simple example in Keras](todo) and expanded into [a
fully-fledged notebook][todo] that explains many details step by
step. I've reproduced it below.

```
from __future__ import print_function

import time
from PIL import Image
import numpy as np

from keras import backend
from keras.models import Model
from keras.applications.vgg16 import VGG16

from scipy.optimize import fmin_l_bfgs_b
from scipy.misc import imsave
```

#### Load and preprocess the content and style images

Our first task is to load the content and style images. Note that the
content image we're working with is not particularly high quality, but
the output we'll arrive at the end of this process still looks really
good.

```
height = 512
width = 512

content_image_path = 'images/hugo.jpg'
content_image = Image.open(content_image_path)
content_image = content_image.resize((height, width))
content_image
```

TODO: Image output

```
style_image_path = 'images/wave.jpg'
style_image = Image.open(style_image_path)
style_image = style_image.resize((height, width))
style_image
```

TODO: Image output

Then, we convert these images into a form suitable for numerical
processing. In particular, we add another dimension (beyond the
classic height x width x 3 dimensions) so that we can later
concatenate the representations of these two images into a common data
structure.

```
content_array = np.asarray(content_image, dtype='float32')
content_array = np.expand_dims(content_array, axis=0)
print(content_array.shape)

style_array = np.asarray(style_image, dtype='float32')
style_array = np.expand_dims(style_array, axis=0)
print(style_array.shape)
```

```
(1, 512, 512, 3)
(1, 512, 512, 3)
```

Before we proceed much further, we need to massage this input data to
match what was done in [Simonyan and Zisserman
(2015)][vgg-simonyan-etal], the paper that introduces the *VGG
Network* model that we're going to use shortly.

For this, we need to perform two transformations:

1. Subtract the mean RGB value (computed previously on the [ImageNet
training set][imagenet] and easily obtainable from Google searches)
from each pixel.
2. Flip the ordering of the multi-dimensional array from *RGB* to
*BGR* (the ordering used in the paper).

```
content_array[:, :, :, 0] -= 103.939
content_array[:, :, :, 1] -= 116.779
content_array[:, :, :, 2] -= 123.68
content_array = content_array[:, :, :, ::-1]

style_array[:, :, :, 0] -= 103.939
style_array[:, :, :, 1] -= 116.779
style_array[:, :, :, 2] -= 123.68
style_array = style_array[:, :, :, ::-1]
```

Now we're ready to use these arrays to define variables in Keras'
backend (the TensorFlow graph). We also introduce a placeholder
variable to store the combination image that retains the content of
the content image while incorporating the style of the style image.

```
content_image = backend.variable(content_array)
style_image = backend.variable(style_array)
combination_image = backend.placeholder((1, height, width, 3))
```

Finally, we concatenate all this image data into a single tensor
that's suitable for processing by Keras' VGG16 model.

```
input_tensor = backend.concatenate([content_image,
                                    style_image,
                                    combination_image], axis=0)
```

#### Reuse a model pre-trained for image classification to define loss functions

The core idea introduced by [Gatys et
al. (2015)](https://arxiv.org/abs/1508.06576) is that convolutional
neural networks (CNNs) pre-trained for image classification already
know how to encode perceptual and semantic information about
images. We're going to follow their idea, and use the *feature spaces*
provided by one such model to independently work with content and
style of images.

The original paper uses the 19 layer VGG network model from [Simonyan
and Zisserman (2015)](https://arxiv.org/abs/1409.1556), but we're
going to instead follow [Johnson et
al. (2016)](https://arxiv.org/abs/1603.08155) and use the 16 layer
model (VGG16). There is no noticeable qualitative difference in making
this choice, and we gain a tiny bit in speed.

Also, since we're not interested in the classification problem, we
don't need the fully connected layers or the final softmax
classifier. We only need the part of the model marked in green in the
table below.

{{< figure src="/images/writing/artistic-style-transfer/vgg-architecture.png" title="VGG Network Architectures." >}}

It is trivial for us to get access to this truncated model because
Keras comes with a set of pretrained models, including the VGG16 model
we're interested in. Note that by setting `include_top=False` in the
code below, we don't include any of the fully connected layers.

```
model = VGG16(input_tensor=input_tensor, weights='imagenet',
              include_top=False)
```

As is clear from the table above, the model we're working with has a
lot of layers. Keras has its own names for these layers. Let's make a
list of these names so that we can easily refer to individual layers
later.

```
layers = dict([(layer.name, layer.output) for layer in model.layers])
print layers
```

```
{'block1_conv1': <tf.Tensor 'Relu:0' shape=(3, 512, 512, 64) dtype=float32>,
 'block1_conv2': <tf.Tensor 'Relu_1:0' shape=(3, 512, 512, 64) dtype=float32>,
 'block1_pool': <tf.Tensor 'MaxPool:0' shape=(3, 256, 256, 64) dtype=float32>,
 'block2_conv1': <tf.Tensor 'Relu_2:0' shape=(3, 256, 256, 128) dtype=float32>,
 'block2_conv2': <tf.Tensor 'Relu_3:0' shape=(3, 256, 256, 128) dtype=float32>,
 'block2_pool': <tf.Tensor 'MaxPool_1:0' shape=(3, 128, 128, 128) dtype=float32>,
 'block3_conv1': <tf.Tensor 'Relu_4:0' shape=(3, 128, 128, 256) dtype=float32>,
 'block3_conv2': <tf.Tensor 'Relu_5:0' shape=(3, 128, 128, 256) dtype=float32>,
 'block3_conv3': <tf.Tensor 'Relu_6:0' shape=(3, 128, 128, 256) dtype=float32>,
 'block3_pool': <tf.Tensor 'MaxPool_2:0' shape=(3, 64, 64, 256) dtype=float32>,
 'block4_conv1': <tf.Tensor 'Relu_7:0' shape=(3, 64, 64, 512) dtype=float32>,
 'block4_conv2': <tf.Tensor 'Relu_8:0' shape=(3, 64, 64, 512) dtype=float32>,
 'block4_conv3': <tf.Tensor 'Relu_9:0' shape=(3, 64, 64, 512) dtype=float32>,
 'block4_pool': <tf.Tensor 'MaxPool_3:0' shape=(3, 32, 32, 512) dtype=float32>,
 'block5_conv1': <tf.Tensor 'Relu_10:0' shape=(3, 32, 32, 512) dtype=float32>,
 'block5_conv2': <tf.Tensor 'Relu_11:0' shape=(3, 32, 32, 512) dtype=float32>,
 'block5_conv3': <tf.Tensor 'Relu_12:0' shape=(3, 32, 32, 512) dtype=float32>,
 'block5_pool': <tf.Tensor 'MaxPool_4:0' shape=(3, 16, 16, 512) dtype=float32>,
 'input_1': <tf.Tensor 'concat:0' shape=(3, 512, 512, 3) dtype=float32>}
```

If you stare at the list above, you'll convince yourself that we
covered all items we wanted in the table (the cells marked in
green). Notice also that because we provided Keras with a concrete
input tensor, the various TensorFlow tensors get well-defined shapes.

---

The crux of the paper we're trying to reproduce is that the [style
transfer problem can be posed as an optimisation
problem](https://harishnarayanan.org/writing/artistic-style-transfer/),
where the loss function we want to minimise can be decomposed into
three distinct parts: the *content loss*, the *style loss* and the
*total variation loss*.

The relative importance of these terms are determined by a set of
scalar weights. These are  arbitrary, but the following set have been
chosen after quite a bit of experimentation to find a set that
generates output that's aesthetically pleasing to me.

```
content_weight = 0.025
style_weight = 5.0
total_variation_weight = 1.0
```

We'll now use the feature spaces provided by specific layers of our
model to define these three loss functions. We begin by initialising
the total loss to 0 and adding to it in stages.

```
loss = backend.variable(0.)
```

##### The content loss

For the content loss, we follow Johnson et al. (2016) and draw the
content feature from `block2_conv2`, because the original choice in
Gatys et al. (2015) (`block4_conv2`) loses too much structural
detail. And at least for faces, I find it more aesthetically pleasing
to closely retain the structure of the original content image.

This variation across layers is shown for a couple of examples in the
images below (just mentally replace `reluX_Y` with our Keras notation
`blockX_convY`).

{{< figure src="/images/writing/artistic-style-transfer/content-feature.png" title="Content feature reconstruction." >}}

The content loss is the (scaled, squared) Euclidean distance between
feature representations of the content and combination images.

```
def content_loss(content, combination):
    return backend.sum(backend.square(combination - content))

layer_features = layers['block2_conv2']
content_image_features = layer_features[0, :, :, :]
combination_features = layer_features[2, :, :, :]

loss += content_weight * content_loss(content_image_features,
                                      combination_features)
```

##### The style loss

This is where things start to get a bit intricate.

For the style loss, we first define something called a *Gram
matrix*. The terms of this matrix are proportional to the covariances
of corresponding sets of features, and thus captures information about
which features tend to activate together. By only capturing these
aggregate statistics across the image, they are blind to the specific
arrangement of objects inside the image. This is what allows them to
capture information about style independent of content. (This is not
trivial at all, and I refer you to [a paper that attempts to explain
the idea](https://arxiv.org/abs/1606.01286).)

The Gram matrix can be computed efficiently by reshaping the feature
spaces suitably and taking an outer product.

```
def gram_matrix(x):
    features = backend.batch_flatten(backend.permute_dimensions(x, (2, 0, 1)))
    gram = backend.dot(features, backend.transpose(features))
    return gram
```

The style loss is then the (scaled, squared) Frobenius norm of the difference between the Gram matrices of the style and combination images.

Again, in the following code, I've chosen to go with the style
features from layers defined in Johnson et al. (2016) rather than
Gatys et al. (2015) because I find the end results more aesthetically
pleasing. I encourage you to experiment with these choices to see
varying results.

```
def style_loss(style, combination):
    S = gram_matrix(style)
    C = gram_matrix(combination)
    channels = 3
    size = height * width
    return backend.sum(backend.square(S - C)) / (4. * (channels ** 2) * (size ** 2))

feature_layers = ['block1_conv2', 'block2_conv2',
                  'block3_conv3', 'block4_conv3',
                  'block5_conv3']
for layer_name in feature_layers:
    layer_features = layers[layer_name]
    style_features = layer_features[1, :, :, :]
    combination_features = layer_features[2, :, :, :]
    sl = style_loss(style_features, combination_features)
    loss += (style_weight / len(feature_layers)) * sl
```

##### The total variation loss

Now we're back on simpler ground.

If you were to solve the optimisation problem with only the two loss
terms we've introduced so far (style and content), you'll find that
the output is quite noisy. We thus add another term, called the [total
variation loss](http://arxiv.org/abs/1412.0035) (a regularisation
term) that encourages spatial smoothness.

You can experiment with reducing the `total_variation_weight` and play
with the noise-level of the generated image.

```
def total_variation_loss(x):
    a = backend.square(x[:, :height-1, :width-1, :] - x[:, 1:, :width-1, :])
    b = backend.square(x[:, :height-1, :width-1, :] - x[:, :height-1, 1:, :])
    return backend.sum(backend.pow(a + b, 1.25))

loss += total_variation_weight * total_variation_loss(combination_image)
```

## Define needed gradients and solve the optimisation problem

The goal of this journey was to setup an optimisation problem that
aims to solve for a *combination image* that contains the content of
the content image, while having the style of the style image. Now that
we have our input images massaged and our loss function calculators in
place, all we have left to do is define gradients of the total loss
relative to the combination image, and use these gradients to
iteratively improve upon our combination image to minimise the loss.

TODO: Figure out if we can do this at a much higher level by replacing
the `Evaluator` class, "manual" gradient calls and raw calls to
`scipy` with `tensorflow.contrib.opt.ScipyOptimizerInterface`.

We start by defining the gradients.

```
grads = backend.gradients(loss, combination_image)

outputs = [loss]
if type(grads) in {list, tuple}:
    outputs += grads
else:
    outputs.append(grads)

f_outputs = backend.function([combination_image], outputs)
```

We then introduce an `Evaluator` class that computes loss and
gradients in one pass while retrieving them via two separate
functions, `loss` and `grads`. This is done because `scipy.optimize`
requires separate functions for loss and gradients, but computing them
separately would be inefficient.

```
def eval_loss_and_grads(x):
    x = x.reshape((1, height, width, 3))
    outs = f_outputs([x])
    loss_value = outs[0]
    if len(outs[1:]) == 1:
        grad_values = outs[1].flatten().astype('float64')
    else:
        grad_values = np.array(outs[1:]).flatten().astype('float64')
    return loss_value, grad_values

class Evaluator(object):

    def __init__(self):
        self.loss_value = None
        self.grads_values = None

    def loss(self, x):
        assert self.loss_value is None
        loss_value, grad_values = eval_loss_and_grads(x)
        self.loss_value = loss_value
        self.grad_values = grad_values
        return self.loss_value

    def grads(self, x):
        assert self.loss_value is not None
        grad_values = np.copy(self.grad_values)
        self.loss_value = None
        self.grad_values = None
        return grad_values

evaluator = Evaluator()
```

Now we're finally ready to solve our optimisation problem. This
combination image begins its life as a random collection of (valid)
pixels, and we use the
[L-BFGS](https://en.wikipedia.org/wiki/Limited-memory_BFGS) algorithm
(a quasi-Newton algorithm that's significantly quicker to converge
than standard gradient descent) to iteratively improve upon it.

We stop after 10 iterations because the output looks good to me and
the loss stops reducing significantly.

```
x = np.random.uniform(0, 255, (1, height, width, 3)) - 128.

iterations = 10

for i in range(iterations):
    print('Start of iteration', i)
    start_time = time.time()
    x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x.flatten(),
                                     fprime=evaluator.grads, maxfun=20)
    print('Current loss value:', min_val)
    end_time = time.time()
    print('Iteration %d completed in %ds' % (i, end_time - start_time))
```

```
Start of iteration 0
Current loss value: 7.97936e+10
Iteration 0 completed in 365s
Start of iteration 1
Current loss value: 5.56155e+10
Iteration 1 completed in 361s
Start of iteration 2
Current loss value: 4.4483e+10
Iteration 2 completed in 360s
...
Start of iteration 9
Current loss value: 3.56783e+10
Iteration 9 completed in 365s
```

This took a while on my piddly laptop (that isn't GPU-accelerated),
but here is the beautiful output from the last iteration! (Notice that
we need to subject our output image to the inverse of the
transformation we did to our input images before it makes sense.)

```
x = x.reshape((height, width, 3))
x = x[:, :, ::-1]
x[:, :, 0] += 103.939
x[:, :, 1] += 116.779
x[:, :, 2] += 123.68
x = np.clip(x, 0, 255).astype('uint8')

Image.fromarray(x)
```

{{< figure src="/images/writing/artistic-style-transfer/animation.gif" title="Iteratively improving upon the combination image." >}}


## Conclusion

<div class="pure-g">
  <div class="pure-u-1-3">
    <img class="pure-img" src="/images/writing/artistic-style-transfer/gothic.jpg" alt="">
  </div>
  <div class="pure-u-1-3">
    <img class="pure-img" src="/images/writing/artistic-style-transfer/scream.jpg" alt="">
  </div>
  <div class="pure-u-1-3">
    <img class="pure-img" src="/images/writing/artistic-style-transfer/wave.jpg" alt="">
  </div>
  <div class="pure-u-1-3">
    <img class="pure-img" src="/images/writing/artistic-style-transfer/IMG_2407.JPG" alt="">
  </div>
  <div class="pure-u-1-3">
    <img class="pure-img" src="/images/writing/artistic-style-transfer/IMG_2408.JPG" alt="">
  </div>
  <div class="pure-u-1-3">
    <img class="pure-img" src="/images/writing/artistic-style-transfer/IMG_2406.JPG" alt="">
  </div>
  <div class="pure-u-1-3">
    <img class="pure-img" src="/images/writing/artistic-style-transfer/c_hugo_candy_s_gothic_cw_0.025_sw_5_tvw_1_i_9.png" alt="">
  </div>
  <div class="pure-u-1-3">
    <img class="pure-img" src="/images/writing/artistic-style-transfer/c_hugo_candy_s_scream_cw_0.025_sw_5_tvw_1_i_9.png" alt="">
  </div>
  <div class="pure-u-1-3">
    <img class="pure-img" src="/images/writing/artistic-style-transfer/c_hugo_candy_s_wave_cw_0.025_sw_5_tvw_1_i_9.png" alt="">
  </div>
</div>

<div class="pure-g">
  <div class="pure-u-1-4">
    <img class="pure-img" src="/images/writing/artistic-style-transfer/c_hugo_bath_s_block_cw_0.025_sw_5_tvw_5_i_9.png" alt="">
  </div>
  <div class="pure-u-1-4">
    <img class="pure-img" src="/images/writing/artistic-style-transfer/c_hugo_bath_s_forest_cw_0.025_sw_5_tvw_1_i_9.png" alt="">
  </div>
  <div class="pure-u-1-4">
    <img class="pure-img" src="/images/writing/artistic-style-transfer/c_hugo_bath_s_gothic_cw_0.025_sw_5_tvw_0.5_i_9.png" alt="">
  </div>
  <div class="pure-u-1-4">
    <img class="pure-img" src="/images/writing/artistic-style-transfer/c_hugo_bath_s_marilyn_cw_0.025_sw_5_tvw_0.1_i_9.png" alt="">
  </div>
  <div class="pure-u-1-4">
    <img class="pure-img" src="/images/writing/artistic-style-transfer/c_hugo_bath_s_picasso_cw_0.025_sw_5_tvw_1_i_9.png" alt="">
  </div>
  <div class="pure-u-1-4">
    <img class="pure-img" src="/images/writing/artistic-style-transfer/c_hugo_bath_s_scream_cw_0.025_sw_5_tvw_0.5_i_9.png" alt="">
  </div>
  <div class="pure-u-1-4">
    <img class="pure-img" src="/images/writing/artistic-style-transfer/c_hugo_bath_s_starry_night_cw_0.025_sw_5_tvw_0.1_i_9.png" alt="">
  </div>
  <div class="pure-u-1-4">
    <img class="pure-img" src="/images/writing/artistic-style-transfer/c_hugo_bath_s_wave_cw_0.025_sw_5_tvw_1_i_9.png" alt="">
  </div>
</div>

- TODO: Show many beautiful examples for our program in action, and
  point to it online. Refer back to first motivating examples from
  Prisma.
- TODO: Talk about how hyperparameters are tuned to improve aesthetic
  quality of the output. Show examples of things that work and things
  that do not.
- TODO: Reiterate some insights.
  - Maybe a giant array of pixels is not the best way of representing
    an image if we wish to understand it better.
  - Some classes of problems are hard to solve with a priori known
    rules, and require learning machines. -> Have them automatically
    discover the representations needed to solve arbitrary problems.
- TODO: Point out that you're now ready to do much more than just this
  problem.
- TODO: Talk about ideas for extension and improvement. Plug the
  subsequent fast style transfer article and *Stylist*.

## Selected references and further reading

1. [A Neural Algorithm of Artistic Style][neural-style-gatys-etal],
   the seminal article
2. [Very Deep Convolutional Networks for Large-Scale Image
   Recognition][vgg-simonyan-etal]
3. [Deep learning][deep-learning-review], a review in Nature
4. [Calculus on Computational Graphs: Backpropagation][backprop-explanation]
5. [The Stanford course on Convolutional Neural Networks][cs231n] and
   [accompanying notes][cs231n-notes]
6. [Our sample implementation on GitHub][neural-style-demo-project]
7. TensorFlow: [Deep CNNs][tensorflow-cnn], [GPU support on
   macOS][tensorflow-gpu-macos]
8. [Keras as a simplified interface to TensorFlow][keras-tensorflow]

[cnn-primer]: #convolutional-neural-networks-from-the-ground-up
[neural-style-implementation]: #concrete-implementation-of-the-artistic-style-transfer-algorithm
[neural-style-notebook]: https://github.com/hnarayanan/stylist/blob/master/core/neural_style_transfer.ipynb
[neural-style-algorithm]: #returning-to-the-style-transfer-problem
[neural-style-demo-project]: https://github.com/hnarayanan/stylist
[prisma]: http://prisma-ai.com
[cnn-wikipedia]: https://en.wikipedia.org/wiki/Convolutional_neural_network
[neural-style-gatys-etal]: https://arxiv.org/abs/1508.06576
[vgg-simonyan-etal]: https://arxiv.org/abs/1409.1556
[imagenet]: http://image-net.org
[deep-learning-review]: https://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf
[backprop-explanation]: http://colah.github.io/posts/2015-08-Backprop/
[cs231n]: http://cs231n.stanford.edu
[cs231n-notes]: http://cs231n.github.io
[cs231n-softmax-classifier]: http://cs231n.github.io/linear-classify/#softmax-classifier
[tensorflow-cnn]: https://www.tensorflow.org/versions/r0.10/tutorials/deep_cnn/index.html
[tensorflow-gpu-macos]: https://gist.github.com/ageitgey/819a51afa4613649bd18
[tensorflow-tutorial-mnist]: https://www.tensorflow.org/versions/r0.10/tutorials/mnist/beginners/index.html
[mnist-dataset]: http://yann.lecun.com/exdb/mnist/
[keras-tensorflow]: https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html
[cross-entropy]: https://en.wikipedia.org/wiki/Cross_entropy
[cross-entropy-reason]: https://jamesmccaffrey.wordpress.com/2013/11/05/why-you-should-use-cross-entropy-error-instead-of-classification-error-or-mean-squared-error-for-neural-network-classifier-training/
[softmax]: https://en.wikipedia.org/wiki/Softmax_function
[gradient-descent]: https://en.wikipedia.org/wiki/Gradient_descent
[gradient-descent-family]: http://cs231n.github.io/neural-networks-3/#update
[activation-functions]: http://cs231n.github.io/neural-networks-1/#actfun
[todo]: todo
